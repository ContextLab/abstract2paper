{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abstract2paper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5b7qlbyGFW8JRXji6QpHl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ContextLab/abstract2paper/blob/main/resources/abstract2paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAVD2y6e-iZO"
      },
      "source": [
        "# Welcome to *abstract2paper*!\n",
        "Author: [Jeremy R. Manning](http://www.context-lab.com/)\n",
        "\n",
        "## Step right up, step right up!\n",
        "<img src='https://media1.giphy.com/media/mL40PfXA394KA/giphy.gif' width='250px'>\n",
        "\n",
        "**Writing papers got you down?** Come on in, friend!  Give my good ole' Abstract2papers Cure-All a quick try!  Enter your abstract into the little doohicky here, and quicker'n you can blink your eyes<sup>1</sup>, a shiny new paper'll come right out for ya!  What are you waiting for?\n",
        "\n",
        "## How does it work, you ask?\n",
        "\n",
        "Really it's quite simple.  We put in a smidgen of [this](https://huggingface.co/transformers/model_doc/gpt_neo.html) a pinch of [that](https://www.tug.org/texlive/), plus a dab of our special [*secret ingredient*](https://www.youtube.com/watch?v=dQw4w9WgXcQ), and **poof!** that's how the sausage is made.\n",
        "\n",
        "## No really, how does it work?\n",
        "\n",
        "Ok, if you really want to know, all I'm doing here is using the [Hugging Face](https://huggingface.co/) [implementation](https://huggingface.co/transformers/model_doc/gpt_neo.html) of [GPT-Neo](https://github.com/EleutherAI/gpt-neo), which is itself a tweaked version of [GPT-3](https://arxiv.org/abs/2005.14165) that is pre-trained on the [Pile](https://pile.eleuther.ai/) dataset.\n",
        "\n",
        "The text you input is used as a prompt for GPT-Neo; to generate a document containing an additional *n* words, the model simply \"predicts\" the next *n* words that will come after the specified prompt.\n",
        "\n",
        "With a little help from some basic [LaTeX](https://www.latex-project.org/) templates (borrowed from [Overleaf](https://www.overleaf.com)), the document is formatted and compiled into a PDF.\n",
        "\n",
        "## Can I actually use this in real-world applications?\n",
        "\n",
        "<img src='https://media4.giphy.com/media/3o6ozoD1ByqYv7ARIk/giphy.gif' width='250px'>\n",
        "\n",
        "**Doubtful.**  Or at least, probably not...?  It certainly wouldn't be ethical to use this code to generate writing assignments, mass-produce papers or grant applications, etc.  Further, you'll likely find that the text produced using this approach includes stuff that's said in funny (often nonsensical) ways, follows problematic logic, incorporates biases from the training data, and so on.  Of lesser importance, but practical annoyance, you'll also encounter all sorts of formatting issues (although those might be easy to fix manually, and possibly even automatically with some clever tinkering).\n",
        "\n",
        "&nbsp;\n",
        "&nbsp;\n",
        "&nbsp;\n",
        "&nbsp;\n",
        "\n",
        "<sup>1</sup><small>This claim rests on the assumption that you blink *really* slowly.  Depending on how much text you're trying to generate (and how long your prompt is), your paper could take anywhere from a few minutes to several hours to fully congeal.</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ52hti0G13T"
      },
      "source": [
        "# Step 1: Setting up the environment\n",
        "\n",
        "We're going to use the super-convenient [Davos](https://github.com/ContextLab/davos.git) package to manage our dependencies.  We need to install it and import it in order to gain access to the `smuggle` keyword."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_8EF8P93B0H",
        "outputId": "18b1cc8c-2389-4b8a-c16c-ddb2b9b4526f"
      },
      "source": [
        "!pip install git+https://github.com/ContextLab/davos.git\n",
        "import davos"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ContextLab/davos.git\n",
            "  Cloning https://github.com/ContextLab/davos.git to /tmp/pip-req-build-bal3wwbr\n",
            "  Running command git clone -q https://github.com/ContextLab/davos.git /tmp/pip-req-build-bal3wwbr\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: davos\n",
            "  Building wheel for davos (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for davos: filename=davos-0.0.1-cp37-none-any.whl size=37733 sha256=61f8ae58e0e17af53308a9a1405d95ededcbbc84201e9b922cdbfae96dac5780\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0obd54sa/wheels/62/30/bc/79958ce75e105bdcf95c737091e62372429850960d6628e277\n",
            "Successfully built davos\n",
            "Installing collected packages: davos\n",
            "Successfully installed davos-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOzbwY-WHVZg"
      },
      "source": [
        "Next, we'll install Hugging Face's [transformers](https://huggingface.co/transformers/) library and download (and load into memory) the pre-trained GPT-Neo model.  We'll also use a pretrained GPT-2 tokenizer for convenience.\n",
        "\n",
        "NB: When you have access to more RAM, you may want to replace `'EleutherAI/gpt-neo-1.3B'` with `'EleutherAI/gpt-neo-2.7B'` in the second two lines in the cell below.  That will load in a fancier (but larger) model-- with 2.7 billion parameters instead of a \"measly\" 1.3 billion parameters.\n",
        "\n",
        "There's a lot to download (roughly 6GB for the smaller model and 12.5GB for the larger model)-- it'll take a few minutes.  Now would be a good time to track down that coffee refill you've been postponing..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMaklzRv3QjF",
        "outputId": "9479a852-14c4-4966-b1b6-e682bfc4e432"
      },
      "source": [
        "from transformers smuggle GPTNeoForCausalLM, GPT2Tokenizer\n",
        "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/92/6153f4912b84ee1ab53ab45663d23e7cf3704161cb5ef18b0c07e207cef2/transformers-4.7.0-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 23.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 29.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwQr2ziLIUyG"
      },
      "source": [
        "# Now I'm only going to ask this once: we're going to need a little...*information* from you...\n",
        "<img src='https://64.media.tumblr.com/e3c8dea30fdf2e597ce79904e8da3271/tumblr_o2xhqwU0cK1qmob6ro2_500.gif' width=400px>\n",
        "\n",
        "You didn't think it was going to be *that* easy, did you?  Oh...you did?  Well if you want *us* to cooperate, we're going to need a little...information...from you first.  About your paper.  Please make this easy on all of us and don't try to lie.  The machine will know.  The machine *always* knows...\n",
        "\n",
        "Fill in the information below and you'll be well on your way to your auto-generated paper/story/grant application/speech/business plan.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amOYPyewDLtE"
      },
      "source": [
        "# credit: https://www.gutenberg.org/files/11/11-h/11-h.htm\n",
        "title = 'Alice\\'s Adventures in Wonderland'\n",
        "authors = 'Lewis Carroll'\n",
        "text = 'Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once ' \\\n",
        "       'or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, ' \\\n",
        "       '“and what is the use of a book,” thought Alice “without pictures or conversations?”\\n\\nSo she was considering ' \\\n",
        "       'in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the ' \\\n",
        "       'pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, ' \\\n",
        "       'when suddenly a White Rabbit with pink eyes ran close by her.\\n\\nThere was nothing so very remarkable in ' \\\n",
        "       'that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, “Oh dear! Oh dear! ' \\\n",
        "       'I shall be late!” (when she thought it over afterwards, it occurred to her that she ought to have wondered at ' \\\n",
        "       'this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its ' \\\n",
        "       'waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her ' \\\n",
        "       'mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, ' \\\n",
        "       'and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop ' \\\n",
        "       'down a large rabbit-hole under the hedge.\\n\\nIn another moment down went Alice after it, never once ' \\\n",
        "       'considering how in the world she was to get out again. '\n",
        "length = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiBDPVxLMQMT"
      },
      "source": [
        "The next cell is going to take a while to run.  While you're waiting, just think: it's still better than writing something on your own, isn't it?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rfrKf764v51"
      },
      "source": [
        "ids = tokenizer(text, return_tensors='pt')['input_ids']\n",
        "tokens = model.generate(ids, do_sample=True, temperature=0.9, max_length=length)\n",
        "gen_text = tokenizer.batch_decode(tokens)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUfWHWA8ILSP"
      },
      "source": [
        "And finally, we'll get a tex-live installation going in our Colaboratory environment and download a template for generating the final document.  (First we need to remove the model and tokenizer from RAM so that Colaboratory doesn't hit its memory limit and crash.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk-ZZeDfM8SJ"
      },
      "source": [
        "# memory cleanup\n",
        "import gc, os\n",
        "del model, tokenizer  # you can comment out this line if you're not\n",
        "                      # running this on a memory-limited machine\n",
        "gc.collect()          # remove the (now unused) variables from memory\n",
        "\n",
        "# install tex-live\n",
        "!apt-get install texlive-latex-recommended\n",
        "\n",
        "# download template\n",
        "!git clone https://github.com/ContextLab/abstract2paper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgat6lZRCV1y"
      },
      "source": [
        "def texer(template, outfile='auto.tex', **kwargs):\n",
        "  with open(template, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "  x = []\n",
        "  for line in lines:\n",
        "    for k, v in kwargs.items():\n",
        "      line = line.replace(f'<{k}>', str(v))\n",
        "    x.append(line.replace(' & ', ' \\& ').replace('%', '\\%'))\n",
        "  \n",
        "  try:\n",
        "    with open(outfile, 'w+') as f:\n",
        "      f.write('\\n'.join(x))\n",
        "  \n",
        "    os.system(f'pdflatex {outfile}')\n",
        "    os.system('rm *.log *.aux')\n",
        "\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  return '\\n'.join(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZPYsfDDQ-q2"
      },
      "source": [
        "Create a PDF containing your auto-generated document..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73tadb3xQD_t"
      },
      "source": [
        "source = texer(os.path.join('abstract2paper', 'resources', 'template.tex'), TITLE=title, AUTHOR=authors+'\\\\\\\\Augmented by GPT-Neo', GEN_TEXT=gen_text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}